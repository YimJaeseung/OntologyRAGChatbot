import os
import re
import uuid
import json
import pandas as pd
import pdfplumber
from datetime import datetime
from typing import List, Dict, Optional

# TypeDB 3.7 í˜¸í™˜ ì„í¬íŠ¸
from typedb.driver import TypeDB, TransactionType, Credentials, DriverOptions
from opensearchpy import OpenSearch
from openai import OpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ---------------------------------------------------------
# 1. Schema Manager: ë™ì ìœ¼ë¡œ L3 íƒ€ì…ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
# ---------------------------------------------------------

class SchemaManager:
    def __init__(self, uri, db_name):
        self.uri = uri
        self.db_name = db_name
        # ì¸ì¦ ì •ë³´ ë° ì˜µì…˜ ì„¤ì •
        self.creds = Credentials("admin", "password")
        self.opts = DriverOptions(is_tls_enabled=False)
        
        # schemal.tqlì— ì •ì˜ëœ L2 Entity ëª©ë¡
        self.valid_parents = {
            "equipment", "component", "sensor", "site", "zone",
            "document-file", "content-unit",
            "engineer", "operator", "manager",
            "fault", "alarm", "maintenance-activity"
        }
        self._known_types = set(self.valid_parents)

    def sanitize_type_name(self, name: str) -> str:
        slug = name.lower()
        slug = re.sub(r'[^a-z0-9\s-]', '', slug)
        slug = re.sub(r'\s+', '-', slug)
        return slug

    def ensure_l3_type(self, l3_name: str, l2_parent: str) -> str:
        slug_l3 = self.sanitize_type_name(l3_name)
        
        if slug_l3 in self._known_types or slug_l3 == l2_parent:
            return slug_l3

        if l2_parent not in self.valid_parents:
            print(f"âš ï¸ Invalid parent '{l2_parent}'. Fallback to 'document-file'")
            l2_parent = "document-file"

        # TypeDB 3.7 í‘œì¤€: driver -> transaction
        with TypeDB.driver(self.uri, self.creds, self.opts) as driver:
            # 1. ì¡´ì¬ í™•ì¸ (ì¿¼ë¦¬ ë°©ì‹)
            with driver.transaction(self.db_name, TransactionType.READ) as tx:
                try:
                    # í•´ë‹¹ íƒ€ì…ì´ ì¡´ì¬í•˜ëŠ”ì§€ concepts APIë¡œ í™•ì¸
                    if tx.concepts.get_entity_type(slug_l3).resolve():
                        return slug_l3
                except Exception:
                    pass # íƒ€ì…ì´ ì—†ìœ¼ë©´ ì•„ë˜ ì •ì˜ ë¡œì§ìœ¼ë¡œ ì´ë™

            # 2. ì—†ìœ¼ë©´ ì •ì˜ (SCHEMA íŠ¸ëœì­ì…˜)
            print(f"ğŸ†• Defining New L3 Type: '{slug_l3}' (sub {l2_parent})")
            try:
                with driver.transaction(self.db_name, TransactionType.SCHEMA) as tx:
                    define_query = f"define entity {slug_l3}, sub {l2_parent};"
                    tx.query(define_query)
                    tx.commit()
                self._known_types.add(slug_l3)
                return slug_l3
            except Exception as e:
                print(f"âš ï¸ Failed to define type: {e}. Fallback to {l2_parent}")
                return l2_parent

# ---------------------------------------------------------
# 2. Dynamic ETL: íŒŒì¼ ì²˜ë¦¬ ë° ë°ì´í„° ì ì¬
# ---------------------------------------------------------
class DynamicETL:
    def __init__(self):
        self.typedb_uri = os.getenv("TYPEDB_ADDRESS", "localhost:1729")
        self.db_name = os.getenv("TYPEDB_DATABASE", "rag_ontology")
        self.creds = Credentials("admin", "password")
        self.opts = DriverOptions(is_tls_enabled=False)
        
        # [ìµœì í™”] TypeDB ë“œë¼ì´ë²„ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ìœ ì§€í•˜ì—¬ ì¬ì‚¬ìš©
        self.driver = TypeDB.driver(self.typedb_uri, self.creds, self.opts)
        
        self.os_client = OpenSearch(
            hosts=[os.getenv("OPENSEARCH_URL", "http://localhost:9200")],
            http_auth=None, use_ssl=False
        )
        self.index_name = "rag-docs"
        self.llm_client = OpenAI(
            base_url=os.getenv("VLLM_API_URL", "http://100.111.233.70:8000/v1"),
            api_key="EMPTY"
        )
        # SchemaManager ìƒì„± ì‹œ ì˜¬ë°”ë¥¸ ë³€ìˆ˜ ì „ë‹¬
        self.schema_mgr = SchemaManager(self.typedb_uri, self.db_name)
        
        # OpenSearch ì¸ë±ìŠ¤ ì´ˆê¸°í™” (Mapping ì„¤ì •)
        self._initialize_index()

    def _initialize_index(self):
        if not self.os_client.indices.exists(index=self.index_name):
            print(f"âš™ï¸ Creating OpenSearch index '{self.index_name}' with k-NN mapping...")
            body = {
                "settings": {"index.knn": True},
                "mappings": {
                    "properties": {
                        "vector_field": {
                            "type": "knn_vector",
                            "dimension": 1536,  # text-embedding-3-small dimension
                            "method": {"name": "hnsw", "engine": "nmslib"}
                        },
                        "text": {"type": "text"},
                        "chunk_id": {"type": "keyword"}
                    }
                }
            }
            self.os_client.indices.create(index=self.index_name, body=body)
            print("âœ… OpenSearch index created.")

    def close(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        self.driver.close()
        self.os_client.close()

    def get_embedding(self, text: str) -> List[float]:
        try:
            return self.llm_client.embeddings.create(
                input=[text.replace("\n", " ")], 
                model="text-embedding-3-small"
            ).data[0].embedding
        except:
            return [0.0] * 1536 

    def analyze_document_type(self, text_snippet: str) -> dict:
        valid_parents_str = ", ".join(self.schema_mgr.valid_parents)
        prompt = f"""
        Analyze the text snippet from an industrial document.
        Determine the specific 'L3 Type' and its 'L2 Parent' from this list: [{valid_parents_str}].
        Snippet: "{text_snippet[:300]}..."
        Return JSON: {{"l3_name": "string", "l2_parent": "string"}}
        """
        try:
            response = self.llm_client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0
            )
            content = response.choices[0].message.content
            clean_json = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_json)
        except:
            return {"l3_name": "General Doc", "l2_parent": "document-file"}

    def insert_to_typedb(self, tql_query):
        # TypeDB 3.7 í‘œì¤€: driver -> transaction
        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
            tx.query(tql_query)
            tx.commit()

    def insert_to_opensearch(self, chunk_id, text, vector, metadata):
        doc = {
            "chunk_id": chunk_id, "text": text, "vector_field": vector,
            "metadata": metadata, "timestamp": datetime.now()
        }
        self.os_client.index(index=self.index_name, body=doc, id=chunk_id)



    def extract_graph_data(self, text: str) -> Dict:
        """[Level 3] LLMì„ í†µí•œ ì—”í‹°í‹° ë° ê´€ê³„ ì¶”ì¶œ"""
        prompt = f"""
        Extract industrial knowledge from the text.
        Return ONLY a JSON object with this structure:
        {{
          "entities": [{{ "name": "Pump A", "type": "equipment" }}],
          "relations": [{{ "from": "Pump A", "to": "System B", "type": "part-of" }}]
        }}
        Text: "{text[:600]}"
        """
        try:
            response = self.llm_client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={ "type": "json_object" }
            )
            return json.loads(response.choices[0].message.content)
        except:
            return {"entities": [], "relations": []}

    def write_graph_to_typedb(self, tx, chunk_id, graph_data):
        """ì¶”ì¶œëœ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ TypeDBì— ì ì¬"""
        # 1. ì—”í‹°í‹° ìƒì„± ë° ì²­í¬ì™€ ì—°ê²° (mention)
        for ent in graph_data.get("entities", []):
            name = ent['name'].replace('"', "'")
            ent_type = ent['type']
            
            # 1. ì—”í‹°í‹° ì¡´ì¬ í™•ì¸
            check_ent = list(tx.query(f'match $e isa {ent_type}, has name "{name}"; get;'))
            
            # 2. ì—†ìœ¼ë©´ ìƒì„±
            if not check_ent:
                tx.query(f'insert $e isa {ent_type}, has name "{name}";')
            
            # 3. ê´€ê³„ ì—°ê²° 
            link_query = f"""
            match $c isa content-unit, has id-chunk-id "{chunk_id}";
                  $e isa {ent_type}, has name "{name}";
            insert (source: $c, target: $e) isa mention;
            """
            tx.query(link_query)

        # 2. ê´€ê³„ ìƒì„± (part-of, monitors ë“±)
        for rel in graph_data.get("relations", []):
            rel_type = rel['type'] 
            from_name = rel['from'].replace('"', "'")
            to_name = rel['to'].replace('"', "'")
            
            # schema.tqlì˜ ê´€ê³„ ì •ì˜ì— ë§ì¶° roleì„ ë§¤í•‘í•´ì•¼ í•¨ (ì˜ˆ: assembly)
            # ì—¬ê¸°ì„œëŠ” ë²”ìš©ì ìœ¼ë¡œ source/target í˜¹ì€ parent/child ê´€ê³„ë¥¼ ì‹œë„
            query = f"""
            match 
                $f isa physical-asset, has name "{from_name}";
                $t isa physical-asset, has name "{to_name}";
            insert 
                (child: $f, parent: $t) isa {rel_type};
            """
            try: tx.query(query)
            except: pass

    async def process_file(self, file_content: bytes, filename: str):
        print(f"ğŸ“‚ Processing file: {filename}")
        doc_id = str(uuid.uuid4())
        is_excel = filename.endswith(".xlsx") or filename.endswith(".xls")
        
        temp_path = f"/tmp/{filename}"
        with open(temp_path, "wb") as f:
            f.write(file_content)

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶€ë¶„ (ê¸°ì¡´ê³¼ ë™ì¼)
        if is_excel:
            df = pd.read_excel(temp_path).fillna("")
            snippet = df.head(5).to_string()
            full_text = df.to_string()
        else:
            with pdfplumber.open(temp_path) as pdf:
                full_text = "\n".join([page.extract_text() or "" for page in pdf.pages])
            snippet = full_text[:1000]

        # íƒ€ì… ë¶„ì„ ë° ë¬¸ì„œ ì—”í‹°í‹° ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
        analysis = self.analyze_document_type(snippet)
        analyzed_type = self.schema_mgr.ensure_l3_type(analysis.get("l3_name"), analysis.get("l2_parent"))
        
        now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
        tql_doc = f"insert $d isa {analyzed_type}, has id-doc-id '{doc_id}', has title '{filename}', has created-date {now};"
        self.insert_to_typedb(tql_doc)

        # --- í•µì‹¬ ìˆ˜ì •: íŠ¸ëœì­ì…˜ì„ ì—´ê³  ë£¨í”„ ë‚´ì—ì„œ ê·¸ë˜í”„ ì¶”ì¶œ ìˆ˜í–‰ ---
        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
                if is_excel and df is not None:
                    for idx, row in df.iterrows():
                        row_id = f"{doc_id}_r{idx}"
                        row_json = row.to_json(force_ascii=False)
                        
                        # 1. ë°ì´í„° ì ì¬
                        tx.query(f"""
                            match $d isa {analyzed_type}, has id-doc-id "{doc_id}";
                            insert $r isa table-row, has id-chunk-id "{row_id}", 
                            has content-text "{str(row_json).replace('"', "'")}", 
                            has row-index {idx}, has created-date {now};
                            (container: $d, content: $r) isa containment;
                        """)
                        # 2. ê·¸ë˜í”„ ì¶”ì¶œ ë° ì—°ê²° (Level 3)
                        graph_data = self.extract_graph_data(row_json)
                        self.write_graph_to_typedb(tx, row_id, graph_data)
                        
                        # ë²¡í„° DB ë™ê¸°í™” (ê¸°ì¡´)
                        vector = self.get_embedding(row_json)
                        self.insert_to_opensearch(row_id, row_json, vector, {"doc_id": doc_id})

                else:
                    splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)
                    for idx, text in enumerate(splitter.split_text(full_text)):
                        chunk_id = f"{doc_id}_c{idx}"
                        
                        # 1. ì²­í¬ ì ì¬
                        tx.query(f"""
                            match $d isa {analyzed_type}, has id-doc-id "{doc_id}";
                            insert $c isa text-chunk, has id-chunk-id "{chunk_id}", 
                            has content-text "{text.replace('"', "'")}", 
                            has page-number {idx}, has created-date {now};
                            (container: $d, content: $c) isa containment;
                        """)
                        # 2. ê·¸ë˜í”„ ì¶”ì¶œ ë° ì—°ê²° (Level 3)
                        graph_data = self.extract_graph_data(text)
                        self.write_graph_to_typedb(tx, chunk_id, graph_data)
                        
                        # ë²¡í„° DB ë™ê¸°í™”
                        vector = self.get_embedding(text)
                        self.insert_to_opensearch(chunk_id, text, vector, {"doc_id": doc_id})
                
                tx.commit() # ëª¨ë“  ì²­í¬ì™€ ì¶”ì¶œëœ ì§€ì‹ì„ í•œ ë²ˆì— ì»¤ë°‹

        os.remove(temp_path)
        return {"status": "success", "doc_id": doc_id}