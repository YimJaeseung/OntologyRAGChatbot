import os
import uuid
import asyncio
import json
from datetime import datetime
from typing import List, Dict, Optional

# TypeDB 3.7 í˜¸í™˜ ì„í¬íŠ¸
from typedb.driver import TypeDB, TransactionType, Credentials, DriverOptions
from opensearchpy import OpenSearch
from openai import OpenAI

# [ë¶„ë¦¬ëœ ëª¨ë“ˆ ì„í¬íŠ¸]
from app.schema import SchemaManager
from app.parser import parse_file_content

# ---------------------------------------------------------
# 2. Dynamic ETL: íŒŒì¼ ì²˜ë¦¬ ë° ë°ì´í„° ì ì¬
# ---------------------------------------------------------
class DynamicETL:
    def __init__(self):
        self.typedb_uri = os.getenv("TYPEDB_ADDRESS", "localhost:1729")
        self.db_name = os.getenv("TYPEDB_DATABASE", "rag_ontology")
        self.creds = Credentials("admin", "password")
        self.opts = DriverOptions(is_tls_enabled=False)
        
        # [ìµœì í™”] TypeDB ë“œë¼ì´ë²„ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ìœ ì§€í•˜ì—¬ ì¬ì‚¬ìš©
        self.driver = TypeDB.driver(self.typedb_uri, self.creds, self.opts)
        
        self.os_client = OpenSearch(
            hosts=[os.getenv("OPENSEARCH_URL", "http://localhost:9200")],
            http_auth=None, use_ssl=False
        )
        self.index_name = "rag-docs"
        self.llm_client = OpenAI(
            base_url=os.getenv("VLLM_API_URL", "http://100.111.233.70:8000/v1"),
            api_key="EMPTY"
        )
        # SchemaManager ìƒì„± ì‹œ ì˜¬ë°”ë¥¸ ë³€ìˆ˜ ì „ë‹¬
        self.schema_mgr = SchemaManager(self.driver, self.db_name)
        
        # OpenSearch ì¸ë±ìŠ¤ ì´ˆê¸°í™” (Mapping ì„¤ì •)
        self._initialize_index()

    def _initialize_index(self):
        if not self.os_client.indices.exists(index=self.index_name):
            print(f"âš™ï¸ Creating OpenSearch index '{self.index_name}' with k-NN mapping...")
            body = {
                "settings": {"index.knn": True},
                "mappings": {
                    "properties": {
                        "vector_field": {
                            "type": "knn_vector",
                            "dimension": 1536,  # text-embedding-3-small dimension
                            "method": {"name": "hnsw", "engine": "nmslib"}
                        },
                        "text": {"type": "text"},
                        "chunk_id": {"type": "keyword"}
                    }
                }
            }
            self.os_client.indices.create(index=self.index_name, body=body)
            print("âœ… OpenSearch index created.")

    def close(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        self.driver.close()
        self.os_client.close()

    def get_embedding(self, text: str) -> List[float]:
        try:
            return self.llm_client.embeddings.create(
                input=[text.replace("\n", " ")], 
                model="text-embedding-3-small"
            ).data[0].embedding
        except:
            return [0.0] * 1536 


    def insert_to_typedb(self, tql_query):
        # TypeDB 3.7 í‘œì¤€: driver -> transaction
        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
            tx.query(tql_query)
            tx.commit()

    def insert_to_opensearch(self, chunk_id, text, vector, metadata):
        doc = {
            "chunk_id": chunk_id, "text": text, "vector_field": vector,
            "metadata": metadata, "timestamp": datetime.now()
        }
        self.os_client.index(index=self.index_name, body=doc, id=chunk_id)



    def extract_graph_data(self, text: str) -> Dict:
        """[Level 3] LLMì„ í†µí•œ ì—”í‹°í‹° ë° ê´€ê³„ ì¶”ì¶œ"""
        valid_types = ", ".join(self.schema_mgr.valid_parents)
        prompt = f"""
        Extract industrial knowledge from the text.
        Identify specific entity types (L3) and their parent categories (L2).
        Identify relationships between entities (e.g., connection, part-of, location).
        Parent categories (L2) should be one of: [{valid_types}].
        
        [Constraints]
        - Do NOT extract 'date', 'time', 'level', 'status', 'description' as Entity Types. These are attributes.
        - Do NOT create generic types like 'site-equipment', 'unnamed-level'. Use specific types.
        - 'sub-project' should be classified as 'project'.

        Return ONLY a JSON object with this structure:
        {{
          "entities": [{{ "name": "Pump A", "type": "centrifugal-pump", "parent_type": "equipment" }}],
          "relations": [{{ "from": "Pump A", "to": "System B", "type": "assembly" }}]
        }}
        Text: "{text[:1000]}"
        """
        try:
            response = self.llm_client.chat.completions.create(
                model="Qwen/Qwen2.5-7B-Instruct",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={ "type": "json_object" }
            )
            return json.loads(response.choices[0].message.content)
        except:
            return {"entities": [], "relations": []}

    async def process_file_pipeline(self, file_content: bytes, filename: str):
        """
        [í†µí•© íŒŒì´í”„ë¼ì¸]
        1. íŒŒì¼ íŒŒì‹± & ì²­í‚¹ (In-Memory)
        2. ì§€ì‹ ì¶”ì¶œ (LLM Analysis)
        3. ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ (Schema Transaction)
        4. ë°ì´í„° ì ì¬ (Write Transaction)
        """
        print(f"ğŸ“‚ Processing file: {filename}")
        doc_id = str(uuid.uuid4())
        
        # Step 1: íŒŒì¼ íŒŒì‹± ë° ì²­í‚¹
        raw_chunks = parse_file_content(file_content, filename)
        
        # [OPTIMIZATION] Create embeddings in parallel
        async def create_embedding_task(chunk_text):
            return await asyncio.to_thread(self.get_embedding, chunk_text)

        embedding_tasks = [create_embedding_task(rc['text']) for rc in raw_chunks]
        vectors = await asyncio.gather(*embedding_tasks)
        
        # ì²­í¬ì— ID ë¶€ì—¬ ë° ì„ë² ë”© ìƒì„± (Enrichment)
        chunks = []
        for i, rc in enumerate(raw_chunks):
            chunk_id = f"{doc_id}_{'r' if rc['type']=='table-row' else 'c'}{rc['index']}"
            chunks.append({
                "chunk_id": chunk_id,
                "text": rc['text'],
                "type": rc['type'],
                "vector": vectors[i]
            })
            
        print(f"âœ… Step 1: Parsed {len(chunks)} chunks.")

        # Step 2: ì²­í¬ë³„ ì§€ì‹ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ìƒì—ì„œ ìˆ˜í–‰)
        extracted_data = await self._analyze_chunks(chunks)
        print(f"âœ… Step 2: Extracted {len(extracted_data['entities'])} entities.")

        # Step 3: ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸
        self._update_schema_definitions(extracted_data['entities'], extracted_data['relations'])
        print(f"âœ… Step 3: Schema updated.")

        # Step 4: DB ì ì¬ (TypeDB + OpenSearch)
        self._save_to_db(doc_id, filename, chunks, extracted_data)
        print(f"âœ… Step 4: Data saved to DB.")

        return {"status": "success", "doc_id": doc_id, "chunks": len(chunks), "entities": len(extracted_data['entities'])}

    async def preview_file_analysis(self, file_content: bytes, filename: str):
        """[Admin] 1ë‹¨ê³„: íŒŒì¼ íŒŒì‹± ë° ì§€ì‹ ì¶”ì¶œ (DB ì €ì¥ X)"""
        doc_id = str(uuid.uuid4())
        
        # 1. íŒŒì‹± ë° ì²­í‚¹
        raw_chunks = parse_file_content(file_content, filename)

        # [OPTIMIZATION] Create embeddings in parallel
        async def create_embedding_task(chunk_text):
            return await asyncio.to_thread(self.get_embedding, chunk_text)

        embedding_tasks = [create_embedding_task(rc['text']) for rc in raw_chunks]
        vectors = await asyncio.gather(*embedding_tasks)

        chunks = []
        for i, rc in enumerate(raw_chunks):
            chunk_id = f"{doc_id}_{'r' if rc['type']=='table-row' else 'c'}{rc['index']}"
            chunks.append({
                "chunk_id": chunk_id,
                "text": rc['text'],
                "type": rc['type'],
                "vector": vectors[i] # ë²¡í„° ìƒì„±ì€ ë¯¸ë¦¬ ìˆ˜í–‰
            })

        # 2. ì§€ì‹ ì¶”ì¶œ
        extracted_data = await self._analyze_chunks(chunks)
        
        return {
            "doc_id": doc_id,
            "filename": filename,
            "chunks": chunks,
            "entities": extracted_data['entities'],
            "relations": extracted_data['relations'],
            "links": extracted_data['links']
        }

    def save_analyzed_data(self, data: Dict):
        """[Admin] 2ë‹¨ê³„: ê²€í† ëœ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë°˜ì˜ ë° DB ì €ì¥"""
        # 1. ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸
        self._update_schema_definitions(data['entities'], data.get('relations', []))
        
        # 2. ë°ì´í„° êµ¬ì¡° ì¬ì¡°ë¦½ (save_to_db í˜¸í™˜)
        extracted_data = {
            "entities": data['entities'],
            "relations": data['relations'],
            "links": data['links']
        }
        
        # 3. DB ì €ì¥
        self._save_to_db(
            doc_id=data['doc_id'], 
            filename=data['filename'], 
            chunks=data['chunks'], 
            extracted_data=extracted_data
        )
        return {"status": "saved", "doc_id": data['doc_id']}

    async def _analyze_chunks(self, chunks: List[Dict]) -> Dict:
        """ê° ì²­í¬ì— ëŒ€í•´ LLMì„ í˜¸ì¶œí•˜ì—¬ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œ"""
        
        # [OPTIMIZATION] Run graph extraction in parallel
        async def extract_task(chunk):
            graph_data = await asyncio.to_thread(self.extract_graph_data, chunk['text'])
            return chunk['chunk_id'], graph_data

        tasks = [extract_task(chunk) for chunk in chunks]
        results = await asyncio.gather(*tasks)

        all_entities = {} # name -> {type, parent}
        all_relations = []
        chunk_links = [] # (chunk_id, entity_name)

        for chunk_id, graph_data in results:
            for ent in graph_data.get("entities", []):
                name = ent.get('name')
                if not name: continue
                
                etype = ent.get('type') or "unknown-entity"
                
                # [Filter] ì†ì„±(Attribute) ì„±ê²©ì˜ ë°ì´í„°ê°€ ì—”í‹°í‹°ë¡œ ì¶”ì¶œë˜ëŠ” ê²ƒ ë°©ì§€
                if etype.lower() in {"date", "datetime", "time", "status", "description", "comment", "note", "unknown", "level", "alarm-level", "unnamed-level", "site-equipment"}:
                    continue

                parent = ent.get("parent_type") or "physical-asset"
                all_entities[name] = {"type": etype, "parent": parent}
                chunk_links.append((chunk_id, name))
            
            for rel in graph_data.get("relations", []):
                all_relations.append(rel)
        
        return {
            "entities": all_entities,
            "relations": all_relations,
            "links": chunk_links
        }

    def _update_schema_definitions(self, entities: Dict, relations: List[Dict] = None):
        """ì¶”ì¶œëœ ì—”í‹°í‹° íƒ€ì…ì„ í™•ì¸í•˜ê³  í•„ìš” ì‹œ ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸"""
        # 1. ì—”í‹°í‹° íƒ€ì… ì •ì˜
        for name, info in entities.items():
            # [Fix] Update type with the actual sanitized/renamed type returned by schema manager
            info['type'] = self.schema_mgr.ensure_l3_type(info['type'], info['parent'])
            
        # 2. ê´€ê³„ íƒ€ì… ì •ì˜
        if relations:
            for rel in relations:
                from_name = rel.get('from')
                to_name = rel.get('to')
                rel_type = rel.get('type')
                
                # ì—”í‹°í‹° ëª©ë¡ì—ì„œ íƒ€ì… ì¡°íšŒ
                from_type = entities.get(from_name, {}).get('type')
                to_type = entities.get(to_name, {}).get('type')
                
                if from_type and to_type and rel_type:
                    final_rel_type = self.schema_mgr.ensure_relation_type(rel_type, from_type, to_type)
                    if final_rel_type:
                        rel['type'] = final_rel_type # Update with sanitized/renamed type

    def _save_to_db(self, doc_id: str, filename: str, chunks: List[Dict], extracted_data: Dict):
        """TypeDBì™€ OpenSearchì— ìµœì¢… ë°ì´í„° ì ì¬"""
        now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
        
        # [Fix] íŒŒì¼ëª… ë‚´ ë°±ìŠ¬ë˜ì‹œ ë° ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
        safe_filename = filename.replace('\\', '\\\\').replace("'", "\\'")

        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
            # 1. ë¬¸ì„œ(Document) ìƒì„±
            tx.query(f"insert $d isa document-file, has id-doc-id '{doc_id}', has name '{safe_filename}', has created-date {now};")

            # 2. ì²­í¬(Content Unit) ìƒì„± ë° ë¬¸ì„œ ì—°ê²°
            for chunk in chunks:
                # [Fix] ë°±ìŠ¬ë˜ì‹œ ì´ìŠ¤ì¼€ì´í”„ í›„, í°ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ì¹˜í™˜ (TQL ë¬¸ìì—´ íŒŒì‹± ì˜¤ë¥˜ ë°©ì§€)
                safe_text = chunk['text'].replace('\\', '\\\\').replace('"', "'")
                # TypeDB ì ì¬
                q_chunk = f"""
                match $d isa document-file, has id-doc-id "{doc_id}";
                insert $c isa {chunk['type']}, has id-chunk-id "{chunk['chunk_id']}", 
                has content-text "{safe_text}", has created-date {now};
                (container: $d, content: $c) isa containment;
                """
                tx.query(q_chunk)
                
                # OpenSearch ì ì¬
                self.insert_to_opensearch(chunk['chunk_id'], chunk['text'], chunk['vector'], {"doc_id": doc_id})

            # ì—”í‹°í‹° ìƒì„±
            for name, info in extracted_data['entities'].items():
                safe_name = name.replace('\\', '\\\\').replace('"', "'")
                etype = info['type']
                # ì¡´ì¬ í™•ì¸ í›„ ìƒì„±
                q_check = tx.query(f'match $e isa {etype}, has name "{safe_name}"; fetch {{ "id": $e }};')
                if hasattr(q_check, 'resolve'): q_check = q_check.resolve()
                if not list(q_check):
                    tx.query(f'insert $e isa {etype}, has name "{safe_name}";')

            # ì²­í¬ ì—°ê²° (Mention)
            for cid, name in extracted_data['links']:
                safe_name = name.replace('\\', '\\\\').replace('"', "'")
                etype = extracted_data['entities'][name]['type']
                q_link = f"""
                match $c isa content-unit, has id-chunk-id "{cid}";
                      $e isa {etype}, has name "{safe_name}";
                insert (source: $c, target: $e) isa mention;
                """
                try: tx.query(q_link)
                except: pass

            # ê´€ê³„ ìƒì„±
            for rel in extracted_data['relations']:
                rtype = rel['type']
                fname = rel['from'].replace('\\', '\\\\').replace('"', "'")
                tname = rel['to'].replace('\\', '\\\\').replace('"', "'")
                
                # [Fix] ê´€ê³„ ì €ì¥ ë¡œì§ ìœ ì—°í™” (ì—¬ëŸ¬ ì—­í•  íŒ¨í„´ ì‹œë„)
                queries = []
                
                # Case 1: Assembly / Part-of (part, system)
                if rtype in ['part-of', 'assembly', 'composition']:
                    rtype = 'assembly'
                    queries.append(f'match $f has name "{fname}"; $t has name "{tname}"; insert (part: $f, system: $t) isa {rtype};')
                
                # Case 2: Location (located, location)
                if rtype == 'location':
                    queries.append(f'match $f has name "{fname}"; $t has name "{tname}"; insert (located: $f, place: $t) isa {rtype};')

                # Case 3: Generic Connection (source, target) - Default fallback
                queries.append(f'match $f has name "{fname}"; $t has name "{tname}"; insert (source: $f, target: $t) isa {rtype};')
                
                for q in queries:
                    try: 
                        tx.query(q)
                        break # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                    except: 
                        pass # ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ íŒ¨í„´ ì‹œë„
            
            tx.commit()

    def delete_document(self, doc_id: str):
        """[Admin] ë¬¸ì„œ ë° ê´€ë ¨ ë°ì´í„° ì‚­ì œ"""
        # 1. TypeDB ì‚­ì œ (ë¬¸ì„œ + í¬í•¨ëœ ì²­í¬)
        # ì£¼ì˜: ì—°ê²°ëœ ì—”í‹°í‹°(ì¥ë¹„ ë“±)ëŠ” ë‹¤ë¥¸ ë¬¸ì„œì—ì„œë„ ì“¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚­ì œí•˜ì§€ ì•ŠìŒ
        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
            q_del = f"""
            match $d isa document-file, has id-doc-id "{doc_id}";
            (container: $d, content: $c) isa containment;
            delete $d, $c;
            """
            tx.query(q_del)
            tx.commit()

        # 2. OpenSearch ì‚­ì œ
        query = {
            "query": {
                "term": {
                    "metadata.doc_id.keyword": doc_id
                }
            }
        }
        self.os_client.delete_by_query(index=self.index_name, body=query)
        return {"status": "deleted", "doc_id": doc_id}

    def list_documents(self):
        """[Admin] ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        docs = []
        try:
            with self.driver.transaction(self.db_name, TransactionType.READ) as tx:
                q = 'match $d isa document-file, has name $n, has id-doc-id $id, has created-date $date; fetch { "id": $id, "name": $n, "date": $date };'
                results = tx.query(q)
                if hasattr(results, 'resolve'): results = results.resolve()
                for res in results:
                    # TypeDBJSON is dict-like, and fetch with JSON structure returns primitive values.
                    doc_id = res.get("id")
                    name = res.get("name")
                    date = res.get("date")
                    if doc_id:
                        # The date is a datetime object, so we convert it to a string for JSON serialization.
                        docs.append({"id": doc_id, "name": name, "date": str(date)})
            
            print(f"ğŸ“„ Listed {len(docs)} documents.")
        except Exception as e:
            print(f"âš ï¸ Error listing documents: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ë©ˆì¶¤ ë°©ì§€
        return docs

    def get_schema_tree(self):
        return self.schema_mgr.get_schema_tree()

    def update_schema_only(self, entities: Dict, relations: List[Dict] = None):
        """[Schema Phase] ë°ì´í„° ì €ì¥ ì—†ì´ ìŠ¤í‚¤ë§ˆë§Œ ì—…ë°ì´íŠ¸"""
        print("ğŸ”„ Starting schema update process...")
        self._update_schema_definitions(entities, relations)
        print("âœ… Schema update process completed.")
        return {"status": "schema_updated", "entity_count": len(entities), "relation_count": len(relations or [])}

    def export_graph_data(self) -> Dict:
        """TypeDBì˜ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°(Ontology)ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        # get_schema_tree()ëŠ” ì´ì œ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì „ì²´ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        schema_tree = self.schema_mgr.get_schema_tree()

        # API ê³„ì•½ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ 'entities'ì™€ 'relations' í‚¤ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        relations = schema_tree.pop("relations", [])
        entities_hierarchy = schema_tree

        return {
            "entities": entities_hierarchy,
            "relations": relations # get_schema_treeì—ì„œ ì´ë¯¸ ì •ë ¬ë¨
        }