import os
import uuid
import asyncio
import json
import time
from datetime import datetime
from typing import List, Dict, Optional

# TypeDB 3.7 í˜¸í™˜ ì„í¬íŠ¸
from typedb.driver import TypeDB, TransactionType, Credentials, DriverOptions
from opensearchpy import OpenSearch
from openai import AsyncOpenAI, APITimeoutError, APIConnectionError, RateLimitError

# [ë¶„ë¦¬ëœ ëª¨ë“ˆ ì„í¬íŠ¸]
from app.schema import SchemaManager
from app.parser import parse_file_content

# ---------------------------------------------------------
# 2. Dynamic ETL: íŒŒì¼ ì²˜ë¦¬ ë° ë°ì´í„° ì ì¬
# ---------------------------------------------------------
class DynamicETL:
    def __init__(self):
        self.typedb_uri = os.getenv("TYPEDB_ADDRESS", "localhost:1729")
        self.db_name = os.getenv("TYPEDB_DATABASE", "rag_ontology")
        self.creds = Credentials("admin", "password")
        self.opts = DriverOptions(is_tls_enabled=False)
        
        # [ìµœì í™”] TypeDB ë“œë¼ì´ë²„ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ìœ ì§€í•˜ì—¬ ì¬ì‚¬ìš©
        self.driver = TypeDB.driver(self.typedb_uri, self.creds, self.opts)
        
        self.os_client = OpenSearch(
            hosts=[os.getenv("OPENSEARCH_URL", "http://localhost:9200")],
            http_auth=None, use_ssl=False
        )
        self.index_name = "rag-docs"
        self.llm_client = AsyncOpenAI(
            base_url=os.getenv("VLLM_API_URL", "http://100.111.233.70:8000/v1"),
            api_key="EMPTY"
        )
        # SchemaManager ìƒì„± ì‹œ ì˜¬ë°”ë¥¸ ë³€ìˆ˜ ì „ë‹¬
        self.schema_mgr = SchemaManager(self.driver, self.db_name)
        
        # OpenSearch ì¸ë±ìŠ¤ ì´ˆê¸°í™” (Mapping ì„¤ì •)
        self._initialize_index()

    def _initialize_index(self):
        if not self.os_client.indices.exists(index=self.index_name):
            print(f"âš™ï¸ Creating OpenSearch index '{self.index_name}' with k-NN mapping...")
            body = {
                "settings": {"index.knn": True},
                "mappings": {
                    "properties": {
                        "vector_field": {
                            "type": "knn_vector",
                            "dimension": 1536,  # text-embedding-3-small dimension
                            "method": {"name": "hnsw", "engine": "nmslib"}
                        },
                        "text": {"type": "text"},
                        "chunk_id": {"type": "keyword"}
                    }
                }
            }
            self.os_client.indices.create(index=self.index_name, body=body)
            print("âœ… OpenSearch index created.")

    def close(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        self.driver.close()
        self.os_client.close()

    async def get_embedding(self, text: str) -> List[float]:
        try:
            response = await self.llm_client.embeddings.create(
                input=[text.replace("\n", " ")], 
                model="text-embedding-3-small"
            )
            return response.data[0].embedding
        except:
            return [0.0] * 1536 


    def insert_to_typedb(self, tql_query):
        # TypeDB 3.7 í‘œì¤€: driver -> transaction
        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
            tx.query(tql_query)
            tx.commit()

    def insert_to_opensearch(self, chunk_id, text, vector, metadata):
        doc = {
            "chunk_id": chunk_id, "text": text, "vector_field": vector,
            "metadata": metadata, "timestamp": datetime.now()
        }
        self.os_client.index(index=self.index_name, body=doc, id=chunk_id)



    async def extract_graph_data(self, text: str) -> Dict:
        """[Level 3] LLMì„ í†µí•œ ì—”í‹°í‹° ë° ê´€ê³„ ì¶”ì¶œ"""
        valid_types = ", ".join(self.schema_mgr.valid_parents)
        
        # [Fix] System Prompt ë¶„ë¦¬ ë° ìŠ¤í‚¤ë§ˆ ëª…ì‹œ
        system_prompt = """
        You are an expert Industrial Knowledge Graph Engineer.
        Extract structured knowledge from the text into a JSON format.

        Output JSON Schema:
        {
          "entities": [ { "name": "string", "type": "string", "parent_type": "string" } ],
          "relations": [ { "from": "string", "to": "string", "type": "string" } ]
        }
        
        IMPORTANT: 'entities' must be a list of OBJECTS (dictionaries), NOT a list of lists.
        """

        # [Prompt Engineering] êµ¬ì„± ìš”ì†Œ ë¶„ë¦¬ (ì¬ì‹œë„ ì‹œ ë‹¨ìˆœí™”ë¥¼ ìœ„í•´)
        definitions = f"""
        [Definitions]
        - **Equipment**: Physical machines and devices (e.g., Pump, Motor, Robot).
        - **Component**: Parts belonging to equipment (e.g., Bearing, Valve, Cable).
        - **Site**: Physical locations (e.g., Factory, Zone, Room).
        - **Operator**: People or teams who operate equipment.
        - **Manager**: People or departments responsible for sites or projects.
        - **Fault**: A malfunction or defect in a component or equipment.
        - **Alarm**: A signal or warning about a fault or an abnormal condition.

        [Rules]
        1. **Entities**: Identify specific L3 types and their L2 parent from: [{valid_types}].
           - Ignore attributes like dates, IDs, status, or generic terms (e.g., "item", "part").
        2. **Relations**: Identify connections like 'assembly' (part-of), 'location' (at), 'responsibility' (by), 'connection'.
        """
        
        example = """
        
        [Example]
        Input: "The Centrifugal Pump (P-101) in Zone A was inspected by the Maintenance Team. Found a crack in the seal."
        Output: {{
          "entities": [
            {{ "name": "P-101", "type": "centrifugal-pump", "parent_type": "equipment" }},
            {{ "name": "Zone A", "type": "zone", "parent_type": "site" }},
            {{ "name": "Maintenance Team", "type": "team", "parent_type": "operator" }},
            {{ "name": "seal", "type": "seal", "parent_type": "component" }},
            {{ "name": "crack", "type": "crack", "parent_type": "fault" }}
          ],
          "relations": [
            {{ "from": "P-101", "to": "Zone A", "type": "location" }},
            {{ "from": "Maintenance Team", "to": "P-101", "type": "responsibility" }},
            {{ "from": "seal", "to": "P-101", "type": "assembly" }},
            {{ "from": "crack", "to": "seal", "type": "caused-by" }}
          ]
        }}
        """
        
        input_data = f"""
        Text: "{text[:2000]}"
        """

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # [Retry Strategy] ì¬ì‹œë„ ì‹œ í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™” (ì˜ˆì‹œ ì œê±°) ë° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°•í™”
                current_system_prompt = system_prompt
                current_user_prompt = definitions + example + input_data
                
                if attempt > 0:
                    current_system_prompt += "\n\nCRITICAL: Your previous response was invalid JSON. Return ONLY the JSON object. Do not include markdown formatting."
                    # ì˜ˆì‹œë¥¼ ì œê±°í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™” (í† í° ì ˆì•½ ë° í˜¼ë€ ë°©ì§€)
                    current_user_prompt = definitions + input_data

                response = await self.llm_client.chat.completions.create(
                    model="Qwen/Qwen2.5-7B-Instruct",
                    messages=[
                        {"role": "system", "content": current_system_prompt},
                        {"role": "user", "content": current_user_prompt}
                    ],
                    temperature=0.1,
                    response_format={ "type": "json_object" }
                )
                data = json.loads(response.choices[0].message.content)
                if not isinstance(data, dict):
                    raise ValueError("LLM returned non-dict JSON")
                return data
            except Exception as e:
                if attempt == max_retries - 1:
                    # print(f"âš ï¸ Extraction failed after {max_retries} attempts: {e}")
                    return {"entities": [], "relations": []}
                # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸° (ë¹„ë™ê¸°)
                await asyncio.sleep(1)

    async def extract_graph_data_batch(self, texts: List[str]) -> Dict:
        """[Level 3] LLMì„ í†µí•œ ì—¬ëŸ¬ í–‰ì˜ ì—”í‹°í‹° ë° ê´€ê³„ ì¼ê´„ ì¶”ì¶œ"""
        valid_types = ", ".join(self.schema_mgr.valid_parents)
        # Combine the JSON strings of rows into a larger JSON array string
        json_array_of_rows = "[" + ",".join(texts) + "]"
        
        # [Fix] System Prompt ë¶„ë¦¬ ë° ìŠ¤í‚¤ë§ˆ ëª…ì‹œ
        system_prompt = """
        You are an expert Industrial Knowledge Graph Engineer.
        Analyze the JSON array of table rows. Consolidate knowledge into a single graph.

        Output JSON Schema:
        {
          "entities": [ { "name": "string", "type": "string", "parent_type": "string" } ],
          "relations": [ { "from": "string", "to": "string", "type": "string" } ]
        }
        
        IMPORTANT: 'entities' must be a list of OBJECTS (dictionaries), NOT a list of lists.
        """

        # [Prompt Engineering] êµ¬ì„± ìš”ì†Œ ë¶„ë¦¬
        definitions = f"""
        [Definitions]
        - **Equipment**: Physical machines and devices (e.g., Pump, Motor, Robot).
        - **Component**: Parts belonging to equipment (e.g., Bearing, Valve, Cable).
        - **Site**: Physical locations (e.g., Factory, Zone, Room).
        - **Operator**: People or teams who operate equipment.
        - **Manager**: People or departments responsible for sites or projects.
        - **Fault**: A malfunction or defect in a component or equipment.
        - **Alarm**: A signal or warning about a fault or an abnormal condition.
        
        [Rules]
        1. **Entities**: Extract L3 types and L2 parents from: [{valid_types}].
           - Ignore: Dates, IDs, Part Numbers, Status, Descriptions.
        2. **Relations**: 'assembly' (part-of), 'location', 'responsibility', 'connection'.
        """

        example = """
        [Example]
        Input: ["{{'Item': 'Pump-A', 'Part': 'Seal', 'Location': 'Room-1'}}"]
        Output: {{
          "entities": [
            {{ "name": "Pump-A", "type": "pump", "parent_type": "equipment" }},
            {{ "name": "Seal", "type": "seal", "parent_type": "component" }},
            {{ "name": "Room-1", "type": "room", "parent_type": "site" }}
          ],
          "relations": [
            {{ "from": "Seal", "to": "Pump-A", "type": "assembly" }},
            {{ "from": "Pump-A", "to": "Room-1", "type": "location" }}
          ]
        }}
        """
        
        input_data = f"""
        JSON Data:
        {json_array_of_rows}
        """
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # [Retry Strategy] ì¬ì‹œë„ ì‹œ í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™”
                current_system_prompt = system_prompt
                current_user_prompt = definitions + example + input_data
                
                if attempt > 0:
                    current_system_prompt += "\n\nCRITICAL: Your previous response was invalid JSON. Return ONLY the JSON object."
                    current_user_prompt = definitions + input_data # ì˜ˆì‹œ ì œê±°

                response = await self.llm_client.chat.completions.create(
                    model="Qwen/Qwen2.5-7B-Instruct",
                    messages=[
                        {"role": "system", "content": current_system_prompt},
                        {"role": "user", "content": current_user_prompt}
                    ],
                    temperature=0.1,
                    response_format={ "type": "json_object" },
                    timeout=120,
                    max_tokens=4096 # [Fix] ì‘ë‹µ ì˜ë¦¼ ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ í† í° ìˆ˜ ëª…ì‹œ
                )
                data = json.loads(response.choices[0].message.content)
                if not isinstance(data, dict):
                    raise ValueError("LLM returned non-dict JSON")
                return data
            except Exception as e:
                print(f"âš ï¸ Batch LLM extraction failed (Attempt {attempt+1}/{max_retries}): {e}. Retrying...", flush=True)
                await asyncio.sleep(2 * (attempt + 1)) # [Fix] ë¹„ë™ê¸° sleepìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€
        
        print(f"âŒ Batch extraction failed after {max_retries} attempts.")
        return {"entities": [], "relations": []}

    async def process_file_pipeline(self, file_content: bytes, filename: str):
        """
        [í†µí•© íŒŒì´í”„ë¼ì¸]
        1. íŒŒì¼ íŒŒì‹± & ì²­í‚¹ (In-Memory)
        2. ì§€ì‹ ì¶”ì¶œ (LLM Analysis)
        3. ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ (Schema Transaction)
        4. ë°ì´í„° ì ì¬ (Write Transaction)
        """
        print(f"ğŸ“‚ Processing file: {filename}")
        doc_id = str(uuid.uuid4())
        
        # Step 1: íŒŒì¼ íŒŒì‹± ë° ì²­í‚¹
        raw_chunks = parse_file_content(file_content, filename)
        
        # [OPTIMIZATION] Create embeddings in parallel
        # [Fix] ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (Semaphore) - ì„ë² ë”©ì€ ë¹„êµì  ë¹ ë¥´ë¯€ë¡œ 20ê°œ
        sem = asyncio.Semaphore(20)
        async def create_embedding_task(chunk_text):
            async with sem:
                return await self.get_embedding(chunk_text)

        embedding_tasks = [create_embedding_task(rc['text']) for rc in raw_chunks]
        vectors = await asyncio.gather(*embedding_tasks)
        
        # ì²­í¬ì— ID ë¶€ì—¬ ë° ì„ë² ë”© ìƒì„± (Enrichment)
        chunks = []
        for i, rc in enumerate(raw_chunks):
            chunk_id = f"{doc_id}_{'r' if rc['type']=='table-row' else 'c'}{rc['index']}"
            chunks.append({
                "chunk_id": chunk_id,
                "text": rc['text'],
                "type": rc['type'],
                "vector": vectors[i]
            })
            
        print(f"âœ… Step 1: Parsed {len(chunks)} chunks.")

        # Step 2: ì²­í¬ë³„ ì§€ì‹ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ìƒì—ì„œ ìˆ˜í–‰)
        extracted_data = await self._analyze_chunks(chunks)
        print(f"âœ… Step 2: Extracted {len(extracted_data['entities'])} entities.")

        # Step 3: ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸
        self._update_schema_definitions(extracted_data['entities'], extracted_data['relations'])
        print(f"âœ… Step 3: Schema updated.")

        # Step 4: DB ì ì¬ (TypeDB + OpenSearch)
        self._save_to_db(doc_id, filename, chunks, extracted_data)
        print(f"âœ… Step 4: Data saved to DB.")

        return {"status": "success", "doc_id": doc_id, "chunks": len(chunks), "entities": len(extracted_data['entities'])}

    async def preview_file_analysis(self, file_content: bytes, filename: str, progress_callback=None):
        """[Admin] 1ë‹¨ê³„: íŒŒì¼ íŒŒì‹± ë° ì§€ì‹ ì¶”ì¶œ (DB ì €ì¥ X)"""
        doc_id = str(uuid.uuid4())
        print(f"  â¡ï¸ Step 1/3: Parsing file '{filename}'...", flush=True)
        
        # 1. íŒŒì‹± ë° ì²­í‚¹
        raw_chunks = parse_file_content(file_content, filename)

        print(f"  â¡ï¸ Step 2/3: Creating embeddings for {len(raw_chunks)} chunks...", flush=True)
        # [OPTIMIZATION] Create embeddings in parallel
        # [Fix] ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
        sem = asyncio.Semaphore(20)
        
        total_embeddings = len(raw_chunks)
        completed_embeddings = 0

        async def create_embedding_task(chunk_text):
            nonlocal completed_embeddings
            async with sem:
                res = await self.get_embedding(chunk_text)
            
            completed_embeddings += 1
            if completed_embeddings % 100 == 0 or completed_embeddings == total_embeddings:
                print(f"    ğŸ”¹ Embedding Progress: {completed_embeddings}/{total_embeddings} ({(completed_embeddings/total_embeddings)*100:.1f}%)", flush=True)
            return res

        try:
            embedding_tasks = [create_embedding_task(rc['text']) for rc in raw_chunks]
            vectors = await asyncio.gather(*embedding_tasks)
        except Exception as e:
            print(f"  âŒ Embedding creation failed: {e}", flush=True)
            raise

        chunks = []
        for i, rc in enumerate(raw_chunks):
            chunk_id = f"{doc_id}_{'r' if rc['type']=='table-row' else 'c'}{rc['index']}"
            chunks.append({
                "chunk_id": chunk_id,
                "text": rc['text'],
                "type": rc['type'],
                "vector": vectors[i] # ë²¡í„° ìƒì„±ì€ ë¯¸ë¦¬ ìˆ˜í–‰
            })

        print(f"  â¡ï¸ Step 3/3: Extracting knowledge from chunks...", flush=True)
        # 2. ì§€ì‹ ì¶”ì¶œ
        extracted_data = await self._analyze_chunks(chunks, progress_callback)
        
        return {
            "doc_id": doc_id,
            "filename": filename,
            "chunks": chunks,
            "entities": extracted_data['entities'],
            "relations": extracted_data['relations'],
            "links": extracted_data['links']
        }

    def save_analyzed_data(self, data: Dict):
        """[Admin] 2ë‹¨ê³„: ê²€í† ëœ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë°˜ì˜ ë° DB ì €ì¥"""
        # 1. ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸
        # [Modified] ì‚¬ìš©ìì˜ ìš”ì²­ìœ¼ë¡œ ì €ì¥ ì‹œ ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ ìƒëµ
        # self._update_schema_definitions(data['entities'], data.get('relations', []))
        
        # 2. ë°ì´í„° êµ¬ì¡° ì¬ì¡°ë¦½ (save_to_db í˜¸í™˜)
        extracted_data = {
            "entities": data['entities'],
            "relations": data['relations'],
            "links": data['links']
        }
        
        # 3. DB ì €ì¥
        self._save_to_db(
            doc_id=data['doc_id'], 
            filename=data['filename'], 
            chunks=data['chunks'], 
            extracted_data=extracted_data
        )
        return {"status": "saved", "doc_id": data['doc_id']}

    async def _analyze_chunks(self, chunks: List[Dict], progress_callback=None) -> Dict:
        """ê° ì²­í¬ì— ëŒ€í•´ LLMì„ í˜¸ì¶œí•˜ì—¬ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œ (ì—‘ì…€ì€ ë°°ì¹˜ ì²˜ë¦¬)"""
        
        tasks = []
        sem = asyncio.Semaphore(5) # [Optimized] ì„œë²„ ë¶€í•˜ ê°ì†Œë¥¼ ìœ„í•´ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¶”ê°€ ê°ì†Œ

        # ì—‘ì…€ í–‰(table-row)ê³¼ ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„ë¦¬
        table_row_chunks = [c for c in chunks if c['type'] == 'table-row']
        other_chunks = [c for c in chunks if c['type'] != 'table-row']

        # 1. ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í¬ëŠ” ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
        for chunk in other_chunks:
            async def extract_single_task(c):
                async with sem:
                    graph_data = await self.extract_graph_data(c['text'])
                # ê²°ê³¼ë¥¼ ( [chunk_id], graph_data ) íŠœí”Œë¡œ í†µì¼
                return [c['chunk_id']], graph_data
            tasks.append(extract_single_task(chunk))

        # 2. ì—‘ì…€ í–‰ì€ ë°°ì¹˜ë¡œ ë¬¶ì–´ ì²˜ë¦¬
        BATCH_SIZE = 10 # [Optimized] ì‘ë‹µ ì˜ë¦¼ ë° íƒ€ì„ì•„ì›ƒ ë°©ì§€ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ì¶”ê°€ ê°ì†Œ
        if table_row_chunks:
            print(f"ğŸ“Š Batching {len(table_row_chunks)} table rows into batches of {BATCH_SIZE}...")
        
        for i in range(0, len(table_row_chunks), BATCH_SIZE):
            batch = table_row_chunks[i:i+BATCH_SIZE]
            batch_texts = [c['text'] for c in batch]
            batch_chunk_ids = [c['chunk_id'] for c in batch]
            
            async def extract_batch_task(texts, chunk_ids):
                async with sem:
                    # ë°°ì¹˜ ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ
                    graph_data = await self.extract_graph_data_batch(texts)
                return chunk_ids, graph_data
            tasks.append(extract_batch_task(batch_texts, batch_chunk_ids))

        # [New] Progress Tracking
        total_tasks = len(tasks)
        completed_tasks = 0
        print(f"ğŸš€ Starting analysis for {total_tasks} tasks...")

        async def wrap_with_progress(task):
            nonlocal completed_tasks
            try:
                res = await task
            except Exception as e:
                print(f"âš ï¸ Task failed in wrap_with_progress: {e}", flush=True)
                res = ([], {"entities": [], "relations": []})
            
            completed_tasks += 1
            if completed_tasks % 5 == 0 or completed_tasks == total_tasks:
                print(f"â³ Analysis Progress: {completed_tasks}/{total_tasks} ({(completed_tasks/total_tasks)*100:.1f}%)", flush=True)
                
                # [WebSocket] Send progress update
                if progress_callback:
                    try:
                        await progress_callback((completed_tasks / total_tasks) * 100, f"Analyzing... {completed_tasks}/{total_tasks}")
                    except Exception as e:
                        print(f"âš ï¸ Progress callback failed: {e}", flush=True)
            return res

        wrapped_tasks = [wrap_with_progress(t) for t in tasks]
        results = await asyncio.gather(*wrapped_tasks)

        all_entities = {} # name -> {type, parent}
        all_relations = []
        chunk_links = [] # (chunk_id, entity_name)

        for chunk_ids, graph_data in results: # chunk_idsëŠ” ì´ì œ ë¦¬ìŠ¤íŠ¸
            # [Fix] graph_dataê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš°(ì˜ˆ: ì—ëŸ¬ ë¬¸ìì—´) ë°©ì–´ ì½”ë“œ ì¶”ê°€
            if not isinstance(graph_data, dict):
                print(f"âš ï¸ Unexpected graph_data type: {type(graph_data)}. Skipping. Value: {str(graph_data)[:100]}", flush=True)
                continue

            for ent in graph_data.get("entities", []):
                # [Fix] LLMì´ ì—”í‹°í‹°ë¥¼ dictê°€ ì•„ë‹Œ list ë“±ìœ¼ë¡œ ì˜ëª» ë°˜í™˜í•˜ëŠ” ê²½ìš° ë°©ì–´
                if not isinstance(ent, dict):
                    print(f"âš ï¸ Unexpected entity format: {type(ent)}. Skipping. Value: {str(ent)[:100]}", flush=True)
                    continue

                name = ent.get('name')
                if not name: continue
                
                etype = ent.get('type') or "unknown-entity"
                
                if etype.lower() in {"date", "datetime", "time", "status", "description", "comment", "note", "unknown", "level", "alarm-level", "unnamed-level", "site-equipment"}:
                    continue

                parent = ent.get("parent_type") or "physical-asset"
                all_entities[name] = {"type": etype, "parent": parent}
                
                # í•´ë‹¹ ì—”í‹°í‹°ë¥¼ ì°¾ì€ ëª¨ë“  ì²­í¬ì™€ ì—°ê²°
                for chunk_id in chunk_ids:
                    chunk_links.append((chunk_id, name))
            
            for rel in graph_data.get("relations", []):
                all_relations.append(rel)
        
        return {
            "entities": all_entities,
            "relations": all_relations,
            "links": chunk_links
        }

    def _update_schema_definitions(self, entities: Dict, relations: List[Dict] = None):
        """ì¶”ì¶œëœ ì—”í‹°í‹° íƒ€ì…ì„ í™•ì¸í•˜ê³  í•„ìš” ì‹œ ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸"""
        # 1. ì—”í‹°í‹° íƒ€ì… ì •ì˜ (Batch Optimization)
        type_pairs = [(info['type'], info['parent']) for info in entities.values()]
        resolved_types = self.schema_mgr.ensure_l3_types_batch(type_pairs)
        
        for name, info in entities.items():
            key = (info['type'], info['parent'])
            if key in resolved_types:
                info['type'] = resolved_types[key]
            
        # 2. ê´€ê³„ íƒ€ì… ì •ì˜
        if relations:
            for rel in relations:
                # [Fix] LLMì´ ê´€ê³„ë¥¼ dictê°€ ì•„ë‹Œ list ë“±ìœ¼ë¡œ ì˜ëª» ë°˜í™˜í•˜ëŠ” ê²½ìš° ë°©ì–´
                if not isinstance(rel, dict):
                    print(f"âš ï¸ Unexpected relation format: {type(rel)}. Skipping. Value: {str(rel)[:100]}", flush=True)
                    continue

                from_name = rel.get('from')
                to_name = rel.get('to')
                rel_type = rel.get('type')
                
                # ì—”í‹°í‹° ëª©ë¡ì—ì„œ íƒ€ì… ì¡°íšŒ
                from_type = entities.get(from_name, {}).get('type')
                to_type = entities.get(to_name, {}).get('type')
                
                if from_type and to_type and rel_type:
                    final_rel_type = self.schema_mgr.ensure_relation_type(rel_type, from_type, to_type)
                    if final_rel_type:
                        rel['type'] = final_rel_type # Update with sanitized/renamed type

    def _save_to_db(self, doc_id: str, filename: str, chunks: List[Dict], extracted_data: Dict):
        """TypeDBì™€ OpenSearchì— ìµœì¢… ë°ì´í„° ì ì¬"""
        now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
        
        # [Fix] íŒŒì¼ëª… ë‚´ ë°±ìŠ¬ë˜ì‹œ ë° ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
        safe_filename = filename.replace('\\', '\\\\').replace("'", "\\'")
        print(f"ğŸ’¾ Saving document '{safe_filename}' (ID: {doc_id}) to TypeDB...")

        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
            # 1. ë¬¸ì„œ(Document) ìƒì„±
            tx.query(f"insert $d isa document-file, has id-doc-id '{doc_id}', has name '{safe_filename}', has created-date {now};")

            # 2. ì²­í¬(Content Unit) ìƒì„± ë° ë¬¸ì„œ ì—°ê²°
            for chunk in chunks:
                # [Fix] ë°±ìŠ¬ë˜ì‹œ ì´ìŠ¤ì¼€ì´í”„ í›„, í°ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ì¹˜í™˜ (TQL ë¬¸ìì—´ íŒŒì‹± ì˜¤ë¥˜ ë°©ì§€)
                safe_text = chunk['text'].replace('\\', '\\\\').replace('"', "'")
                # TypeDB ì ì¬
                q_chunk = f"""
                match $d isa document-file, has id-doc-id "{doc_id}";
                insert $c isa {chunk['type']}, has id-chunk-id "{chunk['chunk_id']}", 
                has content-text "{safe_text}", has created-date {now};
                (container: $d, content: $c) isa containment;
                """
                tx.query(q_chunk)
                
                # OpenSearch ì ì¬
                self.insert_to_opensearch(
                    chunk['chunk_id'], chunk['text'], chunk['vector'], 
                    {"doc_id": doc_id, "filename": filename} # [Fix] doc_idë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
                )

            # ì—”í‹°í‹° ìƒì„±
            for name, info in extracted_data['entities'].items():
                safe_name = name.replace('\\', '\\\\').replace('"', "'")
                etype = info['type']
                # ì¡´ì¬ í™•ì¸ í›„ ìƒì„±
                q_check = tx.query(f'match $e isa {etype}, has name "{safe_name}"; fetch {{ "id": $e }};')
                if hasattr(q_check, 'resolve'): q_check = q_check.resolve()
                if not list(q_check):
                    tx.query(f'insert $e isa {etype}, has name "{safe_name}";')

            # ì²­í¬ ì—°ê²° (Mention)
            for cid, name in extracted_data['links']:
                safe_name = name.replace('\\', '\\\\').replace('"', "'")
                etype = extracted_data['entities'][name]['type']
                q_link = f"""
                match $c isa content-unit, has id-chunk-id "{cid}";
                      $e isa {etype}, has name "{safe_name}";
                insert (source: $c, target: $e) isa mention;
                """
                try: tx.query(q_link)
                except: pass

            # ê´€ê³„ ìƒì„±
            for rel in extracted_data['relations']:
                rtype = rel['type']
                fname = rel['from'].replace('\\', '\\\\').replace('"', "'")
                tname = rel['to'].replace('\\', '\\\\').replace('"', "'")
                
                # [Fix] ê´€ê³„ ì €ì¥ ë¡œì§ ìœ ì—°í™” (ì—¬ëŸ¬ ì—­í•  íŒ¨í„´ ì‹œë„)
                queries = []
                
                # Case 1: Assembly / Part-of (part, system)
                if rtype in ['part-of', 'assembly', 'composition']:
                    rtype = 'assembly'
                    queries.append(f'match $f has name "{fname}"; $t has name "{tname}"; insert (part: $f, system: $t) isa {rtype};')
                
                # Case 2: Location (located, location)
                if rtype == 'location':
                    queries.append(f'match $f has name "{fname}"; $t has name "{tname}"; insert (located: $f, place: $t) isa {rtype};')

                # Case 3: Responsibility (responsible, subject-area)
                if rtype == 'responsibility':
                    queries.append(f'match $f has name "{fname}"; $t has name "{tname}"; insert (responsible: $f, subject-area: $t) isa {rtype};')

                # Case 4: Generic Connection (source, target) - Default fallback
                queries.append(f'match $f has name "{fname}"; $t has name "{tname}"; insert (source: $f, target: $t) isa {rtype};')
                
                for q in queries:
                    try: 
                        tx.query(q)
                        break # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                    except: 
                        pass # ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ íŒ¨í„´ ì‹œë„
            
            tx.commit()
        print(f"âœ… Document '{filename}' saved successfully.")

    def delete_document(self, doc_id: str):
        """[Admin] ë¬¸ì„œ ë° ê´€ë ¨ ë°ì´í„° ì‚­ì œ"""
        print(f"ğŸ—‘ï¸ Deleting document {doc_id}...")
        
        # 1. TypeDB ì‚­ì œ (ë¬¸ì„œ + í¬í•¨ëœ ì²­í¬)
        # ì£¼ì˜: ì—°ê²°ëœ ì—”í‹°í‹°(ì¥ë¹„ ë“±)ëŠ” ë‹¤ë¥¸ ë¬¸ì„œì—ì„œë„ ì“¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚­ì œí•˜ì§€ ì•ŠìŒ
        with self.driver.transaction(self.db_name, TransactionType.WRITE) as tx:
            # 1-1. Mention ê´€ê³„ ì‚­ì œ (Chunkê°€ Sourceì¸ ê²½ìš°)
            q_del_mentions = f"""
            match 
            $d isa document-file, has id-doc-id "{doc_id}";
            $c isa content-unit;
            (container: $d, content: $c) isa containment;
            $m (source: $c) isa mention;
            delete $m;
            """
            try: tx.query(q_del_mentions)
            except Exception as e: print(f"âš ï¸ Error deleting mentions: {e}")

            # 1-2. Chunk ë° Containment ì‚­ì œ
            q_del_chunks = f"""
            match 
            $d isa document-file, has id-doc-id "{doc_id}";
            $c isa content-unit;
            $cont (container: $d, content: $c) isa containment;
            delete $c, $cont;
            """
            try: tx.query(q_del_chunks)
            except Exception as e: print(f"âš ï¸ Error deleting chunks: {e}")

            # 1-3. Document ì‚­ì œ
            q_del_doc = f"""
            match $d isa document-file, has id-doc-id "{doc_id}";
            delete $d;
            """
            try: tx.query(q_del_doc)
            except Exception as e: print(f"âš ï¸ Error deleting document entity: {e}")
            
            tx.commit()

        # 2. OpenSearch ì‚­ì œ
        # [Fix] doc_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ
        try:
            self.os_client.delete_by_query(
                index=self.index_name, body={"query": {"term": {"metadata.doc_id.keyword": doc_id}}}
            )
        except:
            # Fallback for text field
            self.os_client.delete_by_query(
                index=self.index_name, body={"query": {"match": {"metadata.doc_id": doc_id}}}
            )
            
        return {"status": "deleted", "doc_id": doc_id}

    def list_documents(self):
        """[Admin] ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        docs = []
        try:
            with self.driver.transaction(self.db_name, TransactionType.READ) as tx:
                # [Fix] Use attribute projection in fetch to handle optional attributes gracefully
                q = """
                match $d isa document-file;
                fetch { 
                    "id": $d.id-doc-id, 
                    "name": $d.name, 
                    "date": $d.created-date 
                };
                """
                results = tx.query(q)
                if hasattr(results, 'resolve'): results = results.resolve()
                for res in results:
                    # Helper to extract value from potential list or single object
                    def get_val(field):
                        raw = res.get(field)
                        if not raw: return None
                        item = raw[0] if isinstance(raw, list) and raw else raw
                        return item.get("value") if isinstance(item, dict) else item

                    doc_id = get_val("id")
                    name = get_val("name")
                    date = get_val("date")

                    if doc_id:
                        # The date is a datetime object, so we convert it to a string for JSON serialization.
                        docs.append({"id": doc_id, "name": name, "date": str(date) if date else ""})
            
            print(f"ğŸ“„ Listed {len(docs)} documents.")
        except Exception as e:
            print(f"âš ï¸ Error listing documents: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ë©ˆì¶¤ ë°©ì§€
        return docs

    def get_schema_tree(self):
        return self.schema_mgr.get_schema_tree()

    def update_schema_only(self, entities: Dict, relations: List[Dict] = None):
        """[Schema Phase] ë°ì´í„° ì €ì¥ ì—†ì´ ìŠ¤í‚¤ë§ˆë§Œ ì—…ë°ì´íŠ¸"""
        print("ğŸ”„ Starting schema update process...")
        self._update_schema_definitions(entities, relations)
        print("âœ… Schema update process completed.")
        return {"status": "schema_updated", "entity_count": len(entities), "relation_count": len(relations or [])}

    def export_graph_data(self) -> Dict:
        """TypeDBì˜ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°(Ontology)ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        # get_schema_tree()ëŠ” ì´ì œ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì „ì²´ ìŠ¤í‚¤ë§ˆë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        schema_tree = self.schema_mgr.get_schema_tree()

        # API ê³„ì•½ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ 'entities'ì™€ 'relations' í‚¤ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        relations = schema_tree.pop("relations", [])
        entities_hierarchy = schema_tree

        return {
            "entities": entities_hierarchy,
            "relations": relations # get_schema_treeì—ì„œ ì´ë¯¸ ì •ë ¬ë¨
        }