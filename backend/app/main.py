import os
import time
from contextlib import asynccontextmanager
from pydantic import BaseModel
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
# [ìˆ˜ì •] ìµœì‹  ë“œë¼ì´ë²„ êµ¬ì¡°ì— ë§ê²Œ import
from typedb.driver import TypeDB, Credentials, DriverOptions, TransactionType
from app.etl import DynamicETL

# ---------------------------------------------------------
# [1] ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” í•¨ìˆ˜ (Startup Logic)
# ---------------------------------------------------------
def initialize_schema():
    uri = os.getenv("TYPEDB_ADDRESS", "localhost:1729")
    db_name = "rag_ontology"
    schema_path = os.getenv("SCHEMA_PATH", "/init_data/schema.tql")

    print(f"ğŸ”„ Initializing TypeDB at {uri}...")
    print(f"ğŸ“‚ Loading Schema from: {schema_path}")

    # ì¸ì¦ ì •ë³´ ë° ì˜µì…˜ ì„¤ì •
    creds = Credentials("admin", "password")
    opts = DriverOptions(is_tls_enabled=False)

    max_retries = 5
    for attempt in range(max_retries):
        try:
            with TypeDB.driver(uri, creds, opts) as driver:
                # 1. ë°ì´í„°ë² ì´ìŠ¤ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ìƒì„±
                if not driver.databases.contains(db_name):
                    print(f"âœ¨ Creating database '{db_name}'...")
                    driver.databases.create(db_name)
                
                # 2. ìŠ¤í‚¤ë§ˆ íŒŒì¼ ë¡œë“œ ë° ì ì¬ (í•­ìƒ ì‹¤í–‰í•˜ì—¬ ì—…ë°ì´íŠ¸ ë°˜ì˜)
                if os.path.exists(schema_path):
                    print(f"ğŸ“‚ Loading Schema from: {schema_path}")
                    with open(schema_path, "r", encoding="utf-8") as f:
                        schema_query = f.read()
                    
                    # SCHEMA íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì •ì˜ í›„ ë°˜ë“œì‹œ COMMIT
                    with driver.transaction(db_name, TransactionType.SCHEMA) as tx:
                        tx.query(schema_query)
                        tx.commit()
                        print("âœ… Schema applied successfully.")
                else:
                    print(f"âŒ CRITICAL: Schema file missing at {schema_path}")
            
            # ì—°ê²° ë° ì‘ì—… ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
            break
        except Exception as e:
            print(f"âš ï¸ Connection failed on attempt {attempt + 1}/{max_retries}: {e}")
            if attempt == max_retries - 1:
                print("âŒ All attempts to connect to TypeDB failed.")
                raise e
            time.sleep(5)
# ---------------------------------------------------------
# [2] Lifespan ë° App ì„¤ì •
# ---------------------------------------------------------

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ LIFESPAN START: Initializing schema...")
    try:
        initialize_schema()
    except Exception as e:
        print(f"âŒ CRITICAL ERROR DURING INITIALIZATION: {e}")
        raise e

    # Initialize ETL Processor (OpenSearch Connection) with Retry
    global etl_processor
    print("ğŸ”„ Connecting to OpenSearch...")
    for i in range(10):
        try:
            etl_processor = DynamicETL()
            print("âœ… OpenSearch Connected.")
            break
        except Exception as e:
            print(f"âš ï¸ OpenSearch connection failed (Attempt {i+1}/10): {e}")
            if i == 9:
                raise e
            time.sleep(5)
    yield
    print("ğŸš€ LIFESPAN END")

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ETL ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
etl_processor = None

# ---------------------------------------------------------
# [3] RAG Chat API (Hybrid Search)
# ---------------------------------------------------------
class ChatRequest(BaseModel):
    text: str

@app.post("/api/chat")
async def chat(req: ChatRequest):
    question = req.text
    print(f"ğŸ’¬ Received Question: {question}")

    # 1. OpenSearch Vector Search (ìœ ì‚¬ë„ ê²€ìƒ‰)
    vector_results = []
    try:
        query_vec = etl_processor.get_embedding(question)
        os_query = {
            "size": 3,
            "query": {
                "knn": {
                    "vector_field": {
                        "vector": query_vec,
                        "k": 3
                    }
                }
            }
        }
        os_res = etl_processor.os_client.search(index=etl_processor.index_name, body=os_query)
        vector_results = [hit['_source']['text'] for hit in os_res['hits']['hits']]
        print(f"ğŸ” OpenSearch Found: {len(vector_results)} chunks")
    except Exception as e:
        print(f"âš ï¸ OpenSearch Error: {e}")

    # 2. TypeDB Graph Search (í‚¤ì›Œë“œ ê¸°ë°˜ ì—°ê²° íƒìƒ‰)
    graph_results = []
    try:
        # ê°„ë‹¨íˆ ì§ˆë¬¸ì— í¬í•¨ëœ ë‹¨ì–´ë¡œ ì—”í‹°í‹°ë¥¼ ì°¾ê³ , ì—°ê²°ëœ í…ìŠ¤íŠ¸ë¥¼ ì¡°íšŒ
        # (ì‹¤ì œë¡œëŠ” LLMìœ¼ë¡œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ë©´ ë” ì •í™•í•©ë‹ˆë‹¤)
        words = question.split()
        with TypeDB.driver(os.getenv("TYPEDB_ADDRESS", "localhost:1729"), Credentials("admin", "password"), DriverOptions(is_tls_enabled=False)) as driver:
            with driver.transaction("rag_ontology", TransactionType.READ) as tx:
                for word in words:
                    if len(word) < 2: continue
                    # í•´ë‹¹ ë‹¨ì–´ê°€ ì´ë¦„ì— í¬í•¨ëœ ìì‚°(Asset)ê³¼ ì—°ê²°ëœ í…ìŠ¤íŠ¸ ì¡°íšŒ
                    tql = f"""
                    match 
                    $e isa physical-asset, has name $n; 
                    $n contains "{word}";
                    (target: $e, source: $c) isa mention;
                    $c has content-text $text;
                    get $text;
                    """
                    # í™˜ê²½ì— ë§ì¶° tx.query() í•¨ìˆ˜ í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
                    for ans in tx.query(tql):
                        graph_results.append(ans.get("text").as_attribute().get_value())
        print(f"ğŸ•¸ï¸ TypeDB Found: {len(graph_results)} related chunks")
    except Exception as e:
        print(f"âš ï¸ TypeDB Search Error: {e}")

    # 3. Context ê²°í•© ë° LLM ë‹µë³€ ìƒì„±
    context = "\n\n".join(list(set(vector_results + graph_results)))
    
    system_prompt = "You are an industrial AI assistant. Answer based on the context below."
    user_prompt = f"Context:\n{context}\n\nQuestion: {question}"
    
    try:
        response = etl_processor.llm_client.chat.completions.create(
            model="Qwen/Qwen2.5-7B-Instruct",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1
        )
        answer = response.choices[0].message.content
        return {"answer": answer, "context": context}
    except Exception as e:
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error": str(e)}

@app.post("/api/upload")
async def upload_document(file: UploadFile = File(...)):
    try:
        content = await file.read()
        result = await etl_processor.process_file(content, file.filename)
        return result
    except Exception as e:
        print(f"Error processing file: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health_check():
    return {"status": "ok"}